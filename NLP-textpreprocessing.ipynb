{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ddc08-910d-4928-8068-070858e052b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4faf3164-7828-4a60-a4f7-9da69050ab5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...  positive   \n",
      "1  A wonderful little production. The filming tec...  positive   \n",
      "2  I thought this was a wonderful way to spend ti...  positive   \n",
      "3  Basically there's a family where a little boy ...  negative   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "5  Probably my all-time favorite movie, a story o...  positive   \n",
      "6  I sure would like to see a resurrection of a u...  positive   \n",
      "7  This show was an amazing, fresh & innovative i...  negative   \n",
      "8  Encouraged by the positive comments about this...  negative   \n",
      "9  If you like original gut wrenching laughter yo...  positive   \n",
      "\n",
      "                                 preprocessed_review  \n",
      "0  one review mention watch 1 oz episod youll hoo...  \n",
      "1  wonder littl product film techniqu unassum old...  \n",
      "2  thought wonder way spend time hot summer weeke...  \n",
      "3  basic there famili littl boy jake think there ...  \n",
      "4  petter mattei love time money visual stun film...  \n",
      "5  probabl alltim favorit movi stori selfless sac...  \n",
      "6  sure would like see resurrect date seahunt ser...  \n",
      "7  show amaz fresh innov idea 70 first air first ...  \n",
      "8  encourag posit comment film look forward watch...  \n",
      "9  like origin gut wrench laughter like movi youn...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Performs text preprocessing steps.\n",
    "\n",
    "    Args:\n",
    "        text: The input text.\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    \n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\marat\\OneDrive\\Desktop\\2nd year\\project\\IMDB Dataset.csv\")\n",
    "data=data.iloc[0:100]\n",
    "data['review']=data['review'].apply(remove_tags)\n",
    "# Apply preprocessing to the 'review' column\n",
    "data['preprocessed_review'] = data['review'].apply(preprocess_text)\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4deaac3f-d289-47ad-939f-2b3f586cc59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2881c32-227e-4903-bdd2-b66824f0a183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thought wonder way spend time hot summer weekend sit air condit theater watch lightheart comedi plot simplist dialogu witti charact likabl even well bread suspect serial killer may disappoint realiz match point 2 risk addict thought proof woodi allen still fulli control style mani u grown lovethi id laugh one woodi comedi year dare say decad ive never impress scarlet johanson manag tone sexi imag jump right averag spirit young womanthi may crown jewel career wittier devil wear prada interest superman great comedi go see friend'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['preprocessed_review'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba55e7-2426-4908-8b9a-63b7d54654ec",
   "metadata": {},
   "source": [
    "## Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf176cd-fcaf-4122-b26e-0440801c11f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marat\\AppData\\Local\\Temp\\ipykernel_19428\\4204914852.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review']=df['review'].apply(lambda x: x.lower())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen1=\"\"\"Text preprocessing is a crucial step in natural language processing (NLP), where raw text—often noisy and unstructured—is transformed into a format that can be effectively utilized by machine learning algorithms. For example, consider a sentence like: 'The quick brown foxes, were jumping over the lazy dogs!' During preprocessing, one might tokenize the sentence, remove stopwords such as 'the', 'and', or 'were', and normalize text by converting everything to lowercase. Additionally, stemming or lemmatization could be applied to words like 'jumping', 'foxes', and 'dogs' to reduce them to their base forms: 'jump', 'fox', and 'dog'. By applying these techniques, the text becomes easier for models to analyze and process.\"\"\"\n",
    "sen1.lower()\n",
    "import pandas as pd\n",
    "data=pd.read_csv(r\"C:\\Users\\marat\\OneDrive\\Desktop\\2nd year\\project\\IMDB Dataset.csv\")\n",
    "df=data.iloc[:5000]\n",
    "df['review']=df['review'].apply(lambda x: x.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c8e42-6001-4c70-af08-857b7253db7f",
   "metadata": {},
   "source": [
    "## Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33f0611-aebb-4cd9-a920-95986e9f93ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac19d180-de0c-4296-bdd8-cf6142562211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3471619c-2a78-4d11-927c-fe3528f4e541",
   "metadata": {},
   "source": [
    "## Remove Pantuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20cffefc-46e4-4b7b-9f8f-5e8b7b1d24fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marat\\AppData\\Local\\Temp\\ipykernel_19428\\1594149081.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review']=df['review'].apply(remove_tags)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "exclude=string.punctuation\n",
    "\n",
    "def rem_pun(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,'')\n",
    "    return text\n",
    "\n",
    "import re \n",
    "def remove_tags(raw_text):\n",
    "    cle_text=re.sub(re.compile('<.*?>'),'',raw_text)\n",
    "    return cle_text\n",
    "\n",
    "df['review']=df['review'].apply(remove_tags)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84576a96-1d58-4b0f-b59f-2e4b5e66036c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>an interesting slasher film with multiple susp...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>i watched this series when it first came out i...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>once again jet li brings his charismatic prese...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>i rented this movie, after hearing chris gore ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>this was a big disappointment for me. i think ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment\n",
       "0     one of the other reviewers has mentioned that ...  positive\n",
       "1     a wonderful little production. the filming tec...  positive\n",
       "2     i thought this was a wonderful way to spend ti...  positive\n",
       "3     basically there's a family where a little boy ...  negative\n",
       "4     petter mattei's \"love in the time of money\" is...  positive\n",
       "...                                                 ...       ...\n",
       "4995  an interesting slasher film with multiple susp...  negative\n",
       "4996  i watched this series when it first came out i...  positive\n",
       "4997  once again jet li brings his charismatic prese...  positive\n",
       "4998  i rented this movie, after hearing chris gore ...  negative\n",
       "4999  this was a big disappointment for me. i think ...  negative\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rem_pun(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6fe3f-573e-4411-a844-b72d6b01b2b3",
   "metadata": {},
   "source": [
    "## chatword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92cfa7b3-ca72-4efa-b4a1-9f34624fb6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don't care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can't stop laughing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b01bf3c-8272-4c26-8d4d-4842ba37cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for i in text.split():\n",
    "        if i.upper() in chat_words:\n",
    "            new_text.append(chat_words[i.upper()])\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74cb026e-ab4a-4131-9ccb-51e306376d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In My Opinion he is the best\n",
      "For Your Information Mumbai is the capital of Maharashtra\n"
     ]
    }
   ],
   "source": [
    "text = 'IMO he is the best'\n",
    "text1 = 'FYI Mumbai is the capital of Maharashtra'\n",
    "# Calling function\n",
    "print(chat_conversion(text))\n",
    "print(chat_conversion(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302e459-9687-496b-bb1c-069b909b9d3c",
   "metadata": {},
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b3c9a-1f01-48b9-b520-c9c22a0e25df",
   "metadata": {},
   "source": [
    "1. using basic split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ab09370-6de0-4a4a-980d-44f439b02180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text preprocessing is a crucial step in natural language processing (NLP), where raw text—often noisy and unstructured—is transformed into a format that can be effectively utilized by machine learning algorithms',\n",
       " \" For example, consider a sentence like: 'The quick brown foxes, were jumping over the lazy dogs!' During preprocessing, one might tokenize the sentence, remove stopwords such as 'the', 'and', or 'were', and normalize text by converting everything to lowercase\",\n",
       " \" Additionally, stemming or lemmatization could be applied to words like 'jumping', 'foxes', and 'dogs' to reduce them to their base forms: 'jump', 'fox', and 'dog'\",\n",
       " ' By applying these techniques, the text becomes easier for models to analyze and process',\n",
       " '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen1=\"\"\"Text preprocessing is a crucial step in natural language processing (NLP), where raw text—often noisy and unstructured—is transformed into a format that can be effectively utilized by machine learning algorithms. For example, consider a sentence like: 'The quick brown foxes, were jumping over the lazy dogs!' During preprocessing, one might tokenize the sentence, remove stopwords such as 'the', 'and', or 'were', and normalize text by converting everything to lowercase. Additionally, stemming or lemmatization could be applied to words like 'jumping', 'foxes', and 'dogs' to reduce them to their base forms: 'jump', 'fox', and 'dog'. By applying these techniques, the text becomes easier for models to analyze and process.\"\"\"\n",
    "sen1.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22881077-9aeb-435d-ba82-c1d7210a897c",
   "metadata": {},
   "source": [
    "2. Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f3fddac-22dc-44ad-a642-78afb3015666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\marat\\AppData\\Local\\Temp\\ipykernel_11256\\587390606.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  tokens=re.findall(\"[\\w']+\",sen1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'preprocessing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'crucial',\n",
       " 'step',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'NLP',\n",
       " 'where',\n",
       " 'raw',\n",
       " 'text',\n",
       " 'often',\n",
       " 'noisy',\n",
       " 'and',\n",
       " 'unstructured',\n",
       " 'is',\n",
       " 'transformed',\n",
       " 'into',\n",
       " 'a',\n",
       " 'format',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'effectively',\n",
       " 'utilized',\n",
       " 'by',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'For',\n",
       " 'example',\n",
       " 'consider',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'like',\n",
       " \"'The\",\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'foxes',\n",
       " 'were',\n",
       " 'jumping',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dogs',\n",
       " \"'\",\n",
       " 'During',\n",
       " 'preprocessing',\n",
       " 'one',\n",
       " 'might',\n",
       " 'tokenize',\n",
       " 'the',\n",
       " 'sentence',\n",
       " 'remove',\n",
       " 'stopwords',\n",
       " 'such',\n",
       " 'as',\n",
       " \"'the'\",\n",
       " \"'and'\",\n",
       " 'or',\n",
       " \"'were'\",\n",
       " 'and',\n",
       " 'normalize',\n",
       " 'text',\n",
       " 'by',\n",
       " 'converting',\n",
       " 'everything',\n",
       " 'to',\n",
       " 'lowercase',\n",
       " 'Additionally',\n",
       " 'stemming',\n",
       " 'or',\n",
       " 'lemmatization',\n",
       " 'could',\n",
       " 'be',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'words',\n",
       " 'like',\n",
       " \"'jumping'\",\n",
       " \"'foxes'\",\n",
       " 'and',\n",
       " \"'dogs'\",\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'them',\n",
       " 'to',\n",
       " 'their',\n",
       " 'base',\n",
       " 'forms',\n",
       " \"'jump'\",\n",
       " \"'fox'\",\n",
       " 'and',\n",
       " \"'dog'\",\n",
       " 'By',\n",
       " 'applying',\n",
       " 'these',\n",
       " 'techniques',\n",
       " 'the',\n",
       " 'text',\n",
       " 'becomes',\n",
       " 'easier',\n",
       " 'for',\n",
       " 'models',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'and',\n",
       " 'process']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "tokens=re.findall(\"[\\w']+\",sen1)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4696e-66b9-4cfd-8abd-b8f2f7e2c098",
   "metadata": {},
   "source": [
    "# NLTK Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84376fc2-0774-42be-ae91-8146a05f504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92bb0df7-cc9a-4157-9aa8-40d5c9035ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment  \\\n",
      "0  one of the other reviewers has mentioned that ...  positive   \n",
      "1  a wonderful little production. the filming tec...  positive   \n",
      "2  i thought this was a wonderful way to spend ti...  positive   \n",
      "3  basically there's a family where a little boy ...  negative   \n",
      "4  petter mattei's \"love in the time of money\" is...  positive   \n",
      "\n",
      "                                   tokenized_reviews  \n",
      "0  [one, of, the, other, reviewers, has, mentione...  \n",
      "1  [a, wonderful, little, production, ., the, fil...  \n",
      "2  [i, thought, this, was, a, wonderful, way, to,...  \n",
      "3  [basically, there, 's, a, family, where, a, li...  \n",
      "4  [petter, mattei, 's, ``, love, in, the, time, ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marat\\AppData\\Local\\Temp\\ipykernel_19428\\2655165521.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokenized_reviews'] = df['review'].apply(word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "def tokenize_df(df):\n",
    "  df['tokenized_reviews'] = df['review'].apply(word_tokenize)\n",
    "  return df\n",
    "\n",
    "# Tokenize the reviews\n",
    "df = tokenize_df(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80a06372-bda4-45a4-8f2b-5e2ca43ca24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98af29a1-f61c-4a7a-9ba1-09605c2fffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "tok=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd4734c-1300-403e-b6f5-0d6921fe65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=tok(sen1)\n",
    "doc2=tok(sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7b1fcf8-5171-4cc3-924f-76e96aaf8525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\n",
      "email\n",
      "I.D\n",
      "is\n",
      "maratheharshal005@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59e9b8-c098-4cf0-8a00-059623e8a2be",
   "metadata": {},
   "source": [
    "# 4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fc09ae4-cd7e-4a52-9682-39c38111026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd3c0dd8-6681-46fc-91e7-38c6abdf3ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marat\\AppData\\Local\\Temp\\ipykernel_19428\\3396190129.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['stemmed_reviews'] = df['tokenized_reviews'].apply(stem_tokens)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     [one, of, the, other, review, ha, mention, tha...\n",
       "1     [a, wonder, littl, product, ., the, film, tech...\n",
       "2     [i, thought, thi, wa, a, wonder, way, to, spen...\n",
       "3     [basic, there, 's, a, famili, where, a, littl,...\n",
       "4     [petter, mattei, 's, ``, love, in, the, time, ...\n",
       "5     [probabl, my, all-tim, favorit, movi, ,, a, st...\n",
       "6     [i, sure, would, like, to, see, a, resurrect, ...\n",
       "7     [thi, show, wa, an, amaz, ,, fresh, &, innov, ...\n",
       "8     [encourag, by, the, posit, comment, about, thi...\n",
       "9     [if, you, like, origin, gut, wrench, laughter,...\n",
       "10    [phil, the, alien, is, one, of, those, quirki,...\n",
       "11    [i, saw, thi, movi, when, i, wa, about, 12, wh...\n",
       "12    [so, im, not, a, big, fan, of, boll, 's, work,...\n",
       "13    [the, cast, play, shakespeare.shakespear, lost...\n",
       "14    [thi, a, fantast, movi, of, three, prison, who...\n",
       "15    [kind, of, drawn, in, by, the, erot, scene, ,,...\n",
       "16    [some, film, just, simpli, should, not, be, re...\n",
       "17    [thi, movi, made, it, into, one, of, my, top, ...\n",
       "18    [i, rememb, thi, film, ,, it, wa, the, first, ...\n",
       "19    [an, aw, film, !, it, must, have, been, up, ag...\n",
       "20    [after, the, success, of, die, hard, and, it, ...\n",
       "21    [i, had, the, terribl, misfortun, of, have, to...\n",
       "22    [what, an, absolut, stun, movi, ,, if, you, ha...\n",
       "23    [first, of, all, ,, let, 's, get, a, few, thin...\n",
       "24    [thi, wa, the, worst, movi, i, saw, at, worldf...\n",
       "25    [the, karen, carpent, stori, show, a, littl, m...\n",
       "26    [``, the, cell, '', is, an, exot, masterpiec, ...\n",
       "27    [thi, film, tri, to, be, too, mani, thing, all...\n",
       "28    [thi, movi, wa, so, frustrat, ., everyth, seem...\n",
       "29    ['war, movi, ', is, a, hollywood, genr, that, ...\n",
       "Name: stemmed_reviews, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "  stemmer = PorterStemmer()\n",
    "  stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "  return stemmed_tokens\n",
    "    \n",
    "def stem_dataframe(df):\n",
    "  df['stemmed_reviews'] = df['tokenized_reviews'].apply(stem_tokens)\n",
    "  return df\n",
    "\n",
    "# Stem the tokens\n",
    "df = stem_dataframe(df)\n",
    "df.iloc[0:30,3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2305afa-c631-4386-a709-4c89754c53b8",
   "metadata": {},
   "source": [
    "# 5. lemmitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cbf66c2-8b2f-497d-ad66-704090ef0ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\marat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22a011b4-e46e-4d7a-bfcd-53c6e9c4f898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marat\\AppData\\Local\\Temp\\ipykernel_19428\\4274393653.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['lemmatized_reviews'] = df['tokenized_reviews'].apply(lemmatize_tokens)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1     [a, wonderful, little, production, ., the, fil...\n",
       "2     [i, thought, this, be, a, wonderful, way, to, ...\n",
       "3     [basically, there, 's, a, family, where, a, li...\n",
       "4     [petter, mattei, 's, ``, love, in, the, time, ...\n",
       "5     [probably, my, all-time, favorite, movie, ,, a...\n",
       "6     [i, sure, would, like, to, see, a, resurrectio...\n",
       "7     [this, show, be, an, amaze, ,, fresh, &, innov...\n",
       "8     [encourage, by, the, positive, comment, about,...\n",
       "9     [if, you, like, original, gut, wrench, laughte...\n",
       "10    [phil, the, alien, be, one, of, those, quirky,...\n",
       "11    [i, saw, this, movie, when, i, be, about, 12, ...\n",
       "12    [so, im, not, a, big, fan, of, boll, 's, work,...\n",
       "13    [the, cast, played, shakespeare.shakespeare, l...\n",
       "14    [this, a, fantastic, movie, of, three, prisone...\n",
       "15    [kind, of, drawn, in, by, the, erotic, scene, ...\n",
       "16    [some, film, just, simply, should, not, be, re...\n",
       "17    [this, movie, make, it, into, one, of, my, top...\n",
       "18    [i, remember, this, film, ,, it, be, the, firs...\n",
       "19    [an, awful, film, !, it, must, have, be, up, a...\n",
       "20    [after, the, success, of, die, hard, and, it, ...\n",
       "21    [i, have, the, terrible, misfortune, of, have,...\n",
       "22    [what, an, absolutely, stun, movie, ,, if, you...\n",
       "23    [first, of, all, ,, let, 's, get, a, few, thin...\n",
       "24    [this, be, the, bad, movie, i, saw, at, worldf...\n",
       "25    [the, karen, carpenter, story, show, a, little...\n",
       "26    [``, the, cell, '', be, an, exotic, masterpiec...\n",
       "27    [this, film, try, to, be, too, many, thing, al...\n",
       "28    [this, movie, be, so, frustrate, ., everything...\n",
       "29    ['war, movie, ', be, a, hollywood, genre, that...\n",
       "Name: lemmatized_reviews, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "  tag_dict = {\"J\": wordnet.ADJ,\n",
    "              \"N\": wordnet.NOUN, \n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "  return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "  return lemmatized_tokens\n",
    "\n",
    "def lemmatize_dataframe(df):\n",
    "  df['lemmatized_reviews'] = df['tokenized_reviews'].apply(lemmatize_tokens)\n",
    "  return df\n",
    "\n",
    "# Lemmatize the tokens\n",
    "df = lemmatize_dataframe(df[:100])\n",
    "df.iloc[1:30,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dee725e-4ec3-4705-a6b2-398b9f6248dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "thought             think               \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "laughter            laughter            \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      ".                   .                   \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n",
      ".                   .                   \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running thought and eating laughter at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=[\"?:!.,';\"]\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words :\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in  sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b38b83-f7b7-4f04-bcda-c107c00667d4",
   "metadata": {},
   "source": [
    "# feature extraction\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe6f0a-e98c-4da7-bd81-aad7198c10b7",
   "metadata": {},
   "source": [
    "## OneHot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e7e6310-2556-461d-a332-a3526456d028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import itertools\n",
    "\n",
    "document = [\"The\",\"boy\",\"sat\",\"on\",\"the\",\"floor\"]\n",
    "#we have to convert these tokens to a dictionary with the key as word and value as position\n",
    "\n",
    "tokens = [doc.split(\" \") for doc in document]\n",
    "token_chain = itertools.chain.from_iterable(tokens)\n",
    "word_to_id = {token: idx for idx, token in enumerate(set(token_chain))}\n",
    "#word_to_id is our required dictionary\n",
    "#Get the corresponding values for each word\n",
    "token_ids = [[word_to_id[token] for token in toke] for toke in tokens]\n",
    "\n",
    "vec = OneHotEncoder(categories=\"auto\")\n",
    "V = vec.fit_transform(token_ids)\n",
    "print(V.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ee18e-22f7-45b3-974a-d3ee8ff48cc1",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16733862-f936-4586-8990-822cabd7d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14d95607-189d-4e4c-a763-73285092d961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>student learn NLP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NLP learn NLP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>student use BOW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLP use BOW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text  output\n",
       "0  student learn NLP       1\n",
       "1      NLP learn NLP       1\n",
       "2    student use BOW       0\n",
       "3        NLP use BOW       0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'text':['student learn NLP','NLP learn NLP','student use BOW','NLP use BOW'],'output':[1,1,0,0]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "965be359-c1fe-483c-8641-cf9ef564083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4178e5ae-b440-4200-9d8f-adf7b1c89717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulory \n",
      "{'student': 3, 'learn': 1, 'nlp': 2, 'use': 4, 'bow': 0}\n",
      "\n",
      "For each document\n",
      "[[0 1 1 1 0]]\n",
      "[[0 1 2 0 0]]\n",
      "[[1 0 0 1 1]]\n",
      "[[1 0 1 0 1]]\n",
      "\n",
      "For corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bag of words\n",
    "bow=cv.fit_transform(df['text'])\n",
    "#vocab\n",
    "print(\"Vocabulory \")\n",
    "print(cv.vocabulary_)\n",
    "#for each document \n",
    "print(\"\\nFor each document\")\n",
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())\n",
    "print(bow[2].toarray())\n",
    "print(bow[3].toarray())\n",
    "\n",
    "#for whole corpurs\n",
    "print(\"\\nFor corpus\")\n",
    "cv.transform([\"campusx watch and write comment of campusx\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111041dd-fe77-4211-a581-bf44fedcb239",
   "metadata": {},
   "source": [
    "## Term Frequency and Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3bca2df-146c-4d60-b9fe-8bf618ab0456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bow': 1.5108256237659907, 'learn': 1.5108256237659907, 'nlp': 1.2231435513142097, 'student': 1.5108256237659907, 'use': 1.5108256237659907}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text = ['student learn NLP','NLP learn NLP','student use BOW','NLP use BOW']\n",
    "tf = TfidfVectorizer()\n",
    "txt_fit = tf.fit(text)\n",
    "txt_transform = txt_fit.transform(text)\n",
    "idf = tf.idf_\n",
    "print(dict(zip(txt_fit.get_feature_names_out(), idf)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef696828-086f-4452-9c9e-1694787d3000",
   "metadata": {},
   "source": [
    "## N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45308528-4356-40ad-a834-67dae03a961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75858c35-6537-4d9e-9940-0e527fb6d92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample text for n-gram modeling. It is used to demonstrate the concept. It is a text classification technique in NLP\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "import nltk\n",
    "def generate_ngrams(words, n):\n",
    "  ngrams = list(nltk.ngrams(words, n))\n",
    "  return ngrams\n",
    "\n",
    "bigrams = generate_ngrams(words, 2)\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "total_bigrams = sum(bigram_counts.values())\n",
    "\n",
    "def calculate_probability(bigram):\n",
    "  return bigram_counts[bigram] / total_bigrams\n",
    "\n",
    "# Example usage\n",
    "probability_of_is_a = calculate_probability(('is', 'a'))\n",
    "print(probability_of_is_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0c11278-fbbf-47dc-81e9-65876452e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                text  output                           bigrams\n",
      "0  student learn NLP       1  [(student, learn), (learn, NLP)]\n",
      "1      NLP learn NLP       1      [(NLP, learn), (learn, NLP)]\n",
      "2    student use BOW       0      [(student, use), (use, BOW)]\n",
      "3        NLP use BOW       0          [(NLP, use), (use, BOW)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Your DataFrame\n",
    "df = pd.DataFrame({'text':['student learn NLP','NLP learn NLP','student use BOW','NLP use BOW'],'output':[1,1,0,0]})\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "  words = word_tokenize(text)\n",
    "  return list(ngrams(words, n))\n",
    "\n",
    "# Example: Generate bigrams\n",
    "df['bigrams'] = df['text'].apply(lambda x: generate_ngrams(x, 2))\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc542e-f0d1-4bea-a437-5dacbbbe8bf2",
   "metadata": {},
   "source": [
    "# text classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef50de6b-a2ad-4080-beb9-fc578c1a5d9f",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bab110d1-ddbf-4d57-a81d-a7808d23b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\marat\\OneDrive\\Desktop\\2nd year\\project\\IMDB Dataset.csv\")\n",
    "data=data.iloc[0:100]\n",
    "X = data.iloc[:,0:1]\n",
    "y = data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29ca6b55-b84c-4ec3-95e7-44fbc7abe1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "[[10  1]\n",
      " [ 3  6]]\n",
      "The sentiment of the new text is: positive\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\marat\\OneDrive\\Desktop\\2nd year\\project\\IMDB Dataset.csv\")\n",
    "data=data.iloc[0:100]\n",
    "X = data.iloc[:,0:1]\n",
    "y = data['sentiment']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n",
    "\n",
    "# Applying BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_train_bow = cv.fit_transform(X_train['review']).toarray()\n",
    "X_test_bow = cv.transform(X_test['review']).toarray()\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_bow,y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test_bow)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "\n",
    "# Define a function to preprocess and classify new text\n",
    "def classify_text(text):\n",
    "    # Transform new text to Bag-of-Words representation using the same CountVectorizer\n",
    "    text_bow = cv.transform([text]).toarray()\n",
    "    \n",
    "    # Predict sentiment\n",
    "    prediction = gnb.predict(text_bow)\n",
    "    \n",
    "    # Convert numeric prediction back to original label\n",
    "    sentiment = encoder.inverse_transform(prediction)\n",
    "    \n",
    "    return sentiment[0]\n",
    "\n",
    "# Example usage\n",
    "new_text = \"\"\"This a fantastic movie of three prisoners who become famous. One of the actors is george clooney and I'm not a fan but this roll is not bad. Another good thing about the movie is the soundtrack (The man of constant sorrow). I recommend this movie to everybody. Greetings Bart\"\"\"\n",
    "sentiment = classify_text(new_text)\n",
    "print(f\"The sentiment of the new text is: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f73ee104-4134-49dd-8737-dfdd66dfc2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "[[10  1]\n",
      " [ 3  6]]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train_bow,y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test_bow)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "387198df-7a8c-4cc1-8136-47232b886b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "[[11  0]\n",
      " [ 5  4]]\n",
      "The sentiment of the new text is: positive\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_pred = rf.predict(X_test_bow)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# Define a function to preprocess and classify new text\n",
    "def classify_text(text):\n",
    "    # Transform new text to Bag-of-Words representation using the same CountVectorizer\n",
    "    text_bow = cv.transform([text]).toarray()\n",
    "    \n",
    "    # Predict sentiment\n",
    "    prediction = rf.predict(text_bow)\n",
    "    \n",
    "    # Convert numeric prediction back to original label\n",
    "    sentiment = encoder.inverse_transform(prediction)\n",
    "    \n",
    "    return sentiment[0]\n",
    "\n",
    "# Given text\n",
    "new_text = \"\"\"This a fantastic movie of three prisoners who become famous. One of the actors is george clooney and I'm not a fan but this roll is not bad. Another good thing about the movie is the soundtrack (The man of constant sorrow). I recommend this movie to everybody. Greetings Bart\"\"\"\n",
    "\n",
    "# Get sentiment for the new text\n",
    "sentiment = classify_text(new_text)\n",
    "print(f\"The sentiment of the new text is: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6788b5d3-3327-4a22-82b0-8acd48d48e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "[[11  0]\n",
      " [ 6  3]]\n"
     ]
    }
   ],
   "source": [
    "# maximum occuring fearures\n",
    "cv = CountVectorizer(max_features=3000)\n",
    "\n",
    "X_train_bow = cv.fit_transform(X_train['review']).toarray()\n",
    "X_test_bow = cv.transform(X_test['review']).toarray()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_pred = rf.predict(X_test_bow)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6c59479-fde8-42a8-90b5-2d2ffa26678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "0.7\n",
      "[[11  0]\n",
      " [ 6  3]]\n",
      "Naive Bayes\n",
      "0.8\n",
      "[[9 2]\n",
      " [2 7]]\n"
     ]
    }
   ],
   "source": [
    "#usinf TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train['review']).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test['review']).toarray()\n",
    "\n",
    "#random forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_tfidf,y_train)\n",
    "y_pred = rf.predict(X_test_tfidf)\n",
    "print(\"Random Forest\")\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train_tfidf,y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test_tfidf)\n",
    "print(\"Naive Bayes\")\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d4a22-d7b0-46d3-ac10-e62072ebb6de",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d2adb38-4e2c-4c04-b493-cd94e47e56ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n",
      "Confusion Matrix:\n",
      " [[10  1]\n",
      " [ 1  8]]\n",
      "The sentiment of the new text is: negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv(r\"C:\\Users\\marat\\OneDrive\\Desktop\\2nd year\\project\\IMDB Dataset.csv\")\n",
    "data = data.iloc[0:100]\n",
    "X = data.iloc[:, 0:1]\n",
    "y = data['sentiment']\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Apply BoW\n",
    "cv = CountVectorizer()\n",
    "X_train_bow = cv.fit_transform(X_train['review']).toarray()\n",
    "X_test_bow = cv.transform(X_test['review']).toarray()\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = log_reg.predict(X_test_bow)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Define a function to preprocess and classify new text\n",
    "def classify_text(text):\n",
    "    # Transform new text to Bag-of-Words representation using the same CountVectorizer\n",
    "    text_bow = cv.transform([text]).toarray()\n",
    "    \n",
    "    # Predict sentiment\n",
    "    prediction = log_reg.predict(text_bow)\n",
    "    \n",
    "    # Convert numeric prediction back to original label\n",
    "    sentiment = encoder.inverse_transform(prediction)\n",
    "    \n",
    "    return sentiment[0]\n",
    "\n",
    "# Given text\n",
    "new_text = \"\"\"Kind of drawn in by the erotic scenes, only to realize this was one of the most amateurish and unbelievable bits of film I've ever seen. Sort of like a high school film project. What was Rosanna Arquette thinking?? And what was with all those stock characters in that bizarre supposed Midwest town? Pretty hard to get involved with this one. No lessons to be learned from it, no brilliant insights, just stilted and quite ridiculous (but lots of skin, if that intrigues you) videotaped nonsense....What was with the bisexual relationship, out of nowhere, after all the heterosexual encounters. And what was with that absurd dance, with everybody playing their stereotyped roles? Give this one a pass, it's like a million other miles of bad, wasted film, money that could have been spent on starving children or Aids in Africa.....\"\"\"\n",
    "\n",
    "# Get sentiment for the new text\n",
    "sentiment = classify_text(new_text)\n",
    "print(f\"The sentiment of the new text is: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b6505-68b5-4e27-938f-36163d7c3327",
   "metadata": {},
   "source": [
    "# entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc90ff8f-de89-44fc-8012-676cf5459069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.8\n",
      "Naive Bayes Confusion Matrix:\n",
      " [[11  0]\n",
      " [ 4  5]]\n",
      "Logistic Regression Accuracy: 0.9\n",
      "Logistic Regression Confusion Matrix:\n",
      " [[10  1]\n",
      " [ 1  8]]\n",
      "Random Forest Accuracy: 0.8\n",
      "Random Forest Confusion Matrix:\n",
      " [[11  0]\n",
      " [ 4  5]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv(r\"C:\\Users\\marat\\OneDrive\\Desktop\\2nd year\\project\\IMDB Dataset.csv\")\n",
    "data = data.iloc[0:100]\n",
    "X = data.iloc[:, 0:1]\n",
    "y = data['sentiment']\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Apply BoW\n",
    "cv = CountVectorizer()\n",
    "X_train_bow = cv.fit_transform(X_train['review']).toarray()\n",
    "X_test_bow = cv.transform(X_test['review']).toarray()\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_bow, y_train)\n",
    "y_pred_nb = nb.predict(X_test_bow)\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_bow, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test_bow)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "rf.fit(X_train_bow, y_train)\n",
    "y_pred_rf = rf.predict(X_test_bow)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"{model_name} Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(f\"{model_name} Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Print results\n",
    "evaluate_model(y_test, y_pred_nb, \"Naive Bayes\")\n",
    "evaluate_model(y_test, y_pred_log_reg, \"Logistic Regression\")\n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "931d823f-2077-4cb0-ad83-00e237ee03d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss (Entropy): 0.2376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "# Get predicted probabilities for the test set\n",
    "y_prob = model.predict_proba(X_test_bow)\n",
    "# Calculate the cross-entropy loss (entropy)\n",
    "entropy = log_loss(y_test, y_prob)\n",
    "print(f\"Cross-Entropy Loss (Entropy): {entropy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cabaeb-52d3-4a48-885e-e992d4a2fa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "490e90ad-e17f-4fe5-bffb-4c50f86d6657",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\python123\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load pre-trained BERT tokenizer and model\u001b[39;00m\n",
      "File \u001b[1;32mC:\\python123\\Lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32mC:\\python123\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32mC:\\python123\\Lib\\site-packages\\transformers\\utils\\__init__.py:34\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     28\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     ContextManagers,\n\u001b[0;32m     36\u001b[0m     ExplicitEnum,\n\u001b[0;32m     37\u001b[0m     ModelOutput,\n\u001b[0;32m     38\u001b[0m     PaddingStrategy,\n\u001b[0;32m     39\u001b[0m     TensorType,\n\u001b[0;32m     40\u001b[0m     add_model_info_to_auto_map,\n\u001b[0;32m     41\u001b[0m     add_model_info_to_custom_pipelines,\n\u001b[0;32m     42\u001b[0m     cached_property,\n\u001b[0;32m     43\u001b[0m     can_return_loss,\n\u001b[0;32m     44\u001b[0m     expand_dims,\n\u001b[0;32m     45\u001b[0m     filter_out_non_signature_kwargs,\n\u001b[0;32m     46\u001b[0m     find_labels,\n\u001b[0;32m     47\u001b[0m     flatten_dict,\n\u001b[0;32m     48\u001b[0m     infer_framework,\n\u001b[0;32m     49\u001b[0m     is_jax_tensor,\n\u001b[0;32m     50\u001b[0m     is_numpy_array,\n\u001b[0;32m     51\u001b[0m     is_tensor,\n\u001b[0;32m     52\u001b[0m     is_tf_symbolic_tensor,\n\u001b[0;32m     53\u001b[0m     is_tf_tensor,\n\u001b[0;32m     54\u001b[0m     is_torch_device,\n\u001b[0;32m     55\u001b[0m     is_torch_dtype,\n\u001b[0;32m     56\u001b[0m     is_torch_tensor,\n\u001b[0;32m     57\u001b[0m     reshape,\n\u001b[0;32m     58\u001b[0m     squeeze,\n\u001b[0;32m     59\u001b[0m     strtobool,\n\u001b[0;32m     60\u001b[0m     tensor_size,\n\u001b[0;32m     61\u001b[0m     to_numpy,\n\u001b[0;32m     62\u001b[0m     to_py_obj,\n\u001b[0;32m     63\u001b[0m     torch_float,\n\u001b[0;32m     64\u001b[0m     torch_int,\n\u001b[0;32m     65\u001b[0m     transpose,\n\u001b[0;32m     66\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     69\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     70\u001b[0m     HF_MODULES_CACHE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m     try_to_load_from_cache,\n\u001b[0;32m     97\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     99\u001b[0m     ACCELERATE_MIN_VERSION,\n\u001b[0;32m    100\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     torch_only_method,\n\u001b[0;32m    220\u001b[0m )\n",
      "File \u001b[1;32mC:\\python123\\Lib\\site-packages\\transformers\\utils\\generic.py:462\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 462\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_torch_pytree\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_model_output_flatten\u001b[39m(output: ModelOutput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[Any], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_pytree.Context\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32mC:\\python123\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\python123\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_encodings = tokenizer(X_train_bow.tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(X_test_bow.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "import torch\n",
    "class MovieReviewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MovieReviewsDataset(train_encodings, y_train.tolist())\n",
    "test_dataset = MovieReviewsDataset(test_encodings, y_test.tolist())\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d206759-d231-4606-8cf0-4ebe9b2f742d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\python123\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mC:\\python123\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\python123\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb8c3f-58bb-4324-a37f-1d224799e2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f57b1-d36a-4d01-8a7e-45754ef55035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
