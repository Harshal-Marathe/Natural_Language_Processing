{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c2b297-e251-44c1-9daf-9b65f80dadf8",
   "metadata": {},
   "source": [
    "## Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf176cd-fcaf-4122-b26e-0440801c11f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"text preprocessing is a crucial step in natural language processing (nlp), where raw text—often noisy and unstructured—is transformed into a format that can be effectively utilized by machine learning algorithms. for example, consider a sentence like: 'the quick brown foxes, were jumping over the lazy dogs!' during preprocessing, one might tokenize the sentence, remove stopwords such as 'the', 'and', or 'were', and normalize text by converting everything to lowercase. additionally, stemming or lemmatization could be applied to words like 'jumping', 'foxes', and 'dogs' to reduce them to their base forms: 'jump', 'fox', and 'dog'. by applying these techniques, the text becomes easier for models to analyze and process.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen1=\"\"\"Text preprocessing is a crucial step in natural language processing (NLP), where raw text—often noisy and unstructured—is transformed into a format that can be effectively utilized by machine learning algorithms. For example, consider a sentence like: 'The quick brown foxes, were jumping over the lazy dogs!' During preprocessing, one might tokenize the sentence, remove stopwords such as 'the', 'and', or 'were', and normalize text by converting everything to lowercase. Additionally, stemming or lemmatization could be applied to words like 'jumping', 'foxes', and 'dogs' to reduce them to their base forms: 'jump', 'fox', and 'dog'. By applying these techniques, the text becomes easier for models to analyze and process.\"\"\"\n",
    "sen1.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c8e42-6001-4c70-af08-857b7253db7f",
   "metadata": {},
   "source": [
    "## Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e33f0611-aebb-4cd9-a920-95986e9f93ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10061]\n",
      "[nltk_data]     No connection could be made because the target machine\n",
      "[nltk_data]     actively refused it>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac19d180-de0c-4296-bdd8-cf6142562211",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\marat/nltk_data'\n    - 'C:\\\\python123\\\\nltk_data'\n    - 'C:\\\\python123\\\\share\\\\nltk_data'\n    - 'C:\\\\python123\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\marat\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mC:\\python123\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\python123\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\marat/nltk_data'\n    - 'C:\\\\python123\\\\nltk_data'\n    - 'C:\\\\python123\\\\share\\\\nltk_data'\n    - 'C:\\\\python123\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\marat\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m----> 2\u001b[0m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\python123\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mC:\\python123\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mC:\\python123\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\python123\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\marat/nltk_data'\n    - 'C:\\\\python123\\\\nltk_data'\n    - 'C:\\\\python123\\\\share\\\\nltk_data'\n    - 'C:\\\\python123\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\marat\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3471619c-2a78-4d11-927c-fe3528f4e541",
   "metadata": {},
   "source": [
    "## Remove Pantuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20cffefc-46e4-4b7b-9f8f-5e8b7b1d24fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "813349e7-6cb3-49b8-96df-6173866544e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "663376a9-2653-4565-92f8-4e76df87a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_pun(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84576a96-1d58-4b0f-b59f-2e4b5e66036c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string with punctuations for that'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rem_pun(\"string, with! punctuations? for% that$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77438921-a9ae-4353-8bc0-76b312db988f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d302e459-9687-496b-bb1c-069b909b9d3c",
   "metadata": {},
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b3c9a-1f01-48b9-b520-c9c22a0e25df",
   "metadata": {},
   "source": [
    "1. using basic split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab09370-6de0-4a4a-980d-44f439b02180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text preprocessing is a crucial step in natural language processing (NLP), where raw text—often noisy and unstructured—is transformed into a format that can be effectively utilized by machine learning algorithms',\n",
       " \" For example, consider a sentence like: 'The quick brown foxes, were jumping over the lazy dogs!' During preprocessing, one might tokenize the sentence, remove stopwords such as 'the', 'and', or 'were', and normalize text by converting everything to lowercase\",\n",
       " \" Additionally, stemming or lemmatization could be applied to words like 'jumping', 'foxes', and 'dogs' to reduce them to their base forms: 'jump', 'fox', and 'dog'\",\n",
       " ' By applying these techniques, the text becomes easier for models to analyze and process',\n",
       " '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen1=\"\"\"Text preprocessing is a crucial step in natural language processing (NLP), where raw text—often noisy and unstructured—is transformed into a format that can be effectively utilized by machine learning algorithms. For example, consider a sentence like: 'The quick brown foxes, were jumping over the lazy dogs!' During preprocessing, one might tokenize the sentence, remove stopwords such as 'the', 'and', or 'were', and normalize text by converting everything to lowercase. Additionally, stemming or lemmatization could be applied to words like 'jumping', 'foxes', and 'dogs' to reduce them to their base forms: 'jump', 'fox', and 'dog'. By applying these techniques, the text becomes easier for models to analyze and process.\"\"\"\n",
    "sen1.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22881077-9aeb-435d-ba82-c1d7210a897c",
   "metadata": {},
   "source": [
    "2. Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f3fddac-22dc-44ad-a642-78afb3015666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\marat\\AppData\\Local\\Temp\\ipykernel_7728\\587390606.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  tokens=re.findall(\"[\\w']+\",sen1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'preprocessing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'crucial',\n",
       " 'step',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'NLP',\n",
       " 'where',\n",
       " 'raw',\n",
       " 'text',\n",
       " 'often',\n",
       " 'noisy',\n",
       " 'and',\n",
       " 'unstructured',\n",
       " 'is',\n",
       " 'transformed',\n",
       " 'into',\n",
       " 'a',\n",
       " 'format',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'effectively',\n",
       " 'utilized',\n",
       " 'by',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'For',\n",
       " 'example',\n",
       " 'consider',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'like',\n",
       " \"'The\",\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'foxes',\n",
       " 'were',\n",
       " 'jumping',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dogs',\n",
       " \"'\",\n",
       " 'During',\n",
       " 'preprocessing',\n",
       " 'one',\n",
       " 'might',\n",
       " 'tokenize',\n",
       " 'the',\n",
       " 'sentence',\n",
       " 'remove',\n",
       " 'stopwords',\n",
       " 'such',\n",
       " 'as',\n",
       " \"'the'\",\n",
       " \"'and'\",\n",
       " 'or',\n",
       " \"'were'\",\n",
       " 'and',\n",
       " 'normalize',\n",
       " 'text',\n",
       " 'by',\n",
       " 'converting',\n",
       " 'everything',\n",
       " 'to',\n",
       " 'lowercase',\n",
       " 'Additionally',\n",
       " 'stemming',\n",
       " 'or',\n",
       " 'lemmatization',\n",
       " 'could',\n",
       " 'be',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'words',\n",
       " 'like',\n",
       " \"'jumping'\",\n",
       " \"'foxes'\",\n",
       " 'and',\n",
       " \"'dogs'\",\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'them',\n",
       " 'to',\n",
       " 'their',\n",
       " 'base',\n",
       " 'forms',\n",
       " \"'jump'\",\n",
       " \"'fox'\",\n",
       " 'and',\n",
       " \"'dog'\",\n",
       " 'By',\n",
       " 'applying',\n",
       " 'these',\n",
       " 'techniques',\n",
       " 'the',\n",
       " 'text',\n",
       " 'becomes',\n",
       " 'easier',\n",
       " 'for',\n",
       " 'models',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'and',\n",
       " 'process']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "tokens=re.findall(\"[\\w']+\",sen1)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4696e-66b9-4cfd-8abd-b8f2f7e2c098",
   "metadata": {},
   "source": [
    "3. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84376fc2-0774-42be-ae91-8146a05f504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5467c18e-75a0-405e-8073-7cb9d76e71e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'preprocessing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'crucial',\n",
       " 'step',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " ',',\n",
       " 'where',\n",
       " 'raw',\n",
       " 'text—often',\n",
       " 'noisy',\n",
       " 'and',\n",
       " 'unstructured—is',\n",
       " 'transformed',\n",
       " 'into',\n",
       " 'a',\n",
       " 'format',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'effectively',\n",
       " 'utilized',\n",
       " 'by',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " 'consider',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'like',\n",
       " ':',\n",
       " \"'The\",\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'foxes',\n",
       " ',',\n",
       " 'were',\n",
       " 'jumping',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dogs',\n",
       " '!',\n",
       " \"'\",\n",
       " 'During',\n",
       " 'preprocessing',\n",
       " ',',\n",
       " 'one',\n",
       " 'might',\n",
       " 'tokenize',\n",
       " 'the',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'remove',\n",
       " 'stopwords',\n",
       " 'such',\n",
       " 'as',\n",
       " \"'the\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'and\",\n",
       " \"'\",\n",
       " ',',\n",
       " 'or',\n",
       " \"'were\",\n",
       " \"'\",\n",
       " ',',\n",
       " 'and',\n",
       " 'normalize',\n",
       " 'text',\n",
       " 'by',\n",
       " 'converting',\n",
       " 'everything',\n",
       " 'to',\n",
       " 'lowercase',\n",
       " '.',\n",
       " 'Additionally',\n",
       " ',',\n",
       " 'stemming',\n",
       " 'or',\n",
       " 'lemmatization',\n",
       " 'could',\n",
       " 'be',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'words',\n",
       " 'like',\n",
       " \"'jumping\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'foxes\",\n",
       " \"'\",\n",
       " ',',\n",
       " 'and',\n",
       " \"'dogs\",\n",
       " \"'\",\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'them',\n",
       " 'to',\n",
       " 'their',\n",
       " 'base',\n",
       " 'forms',\n",
       " ':',\n",
       " \"'jump\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'fox\",\n",
       " \"'\",\n",
       " ',',\n",
       " 'and',\n",
       " \"'dog\",\n",
       " \"'\",\n",
       " '.',\n",
       " 'By',\n",
       " 'applying',\n",
       " 'these',\n",
       " 'techniques',\n",
       " ',',\n",
       " 'the',\n",
       " 'text',\n",
       " 'becomes',\n",
       " 'easier',\n",
       " 'for',\n",
       " 'models',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'and',\n",
       " 'process',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6046a56b-7074-4dad-bb4c-5f6787c32f45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text preprocessing is a crucial step in natural language processing (NLP), where raw text—often noisy and unstructured—is transformed into a format that can be effectively utilized by machine learning algorithms.',\n",
       " \"For example, consider a sentence like: 'The quick brown foxes, were jumping over the lazy dogs!'\",\n",
       " \"During preprocessing, one might tokenize the sentence, remove stopwords such as 'the', 'and', or 'were', and normalize text by converting everything to lowercase.\",\n",
       " \"Additionally, stemming or lemmatization could be applied to words like 'jumping', 'foxes', and 'dogs' to reduce them to their base forms: 'jump', 'fox', and 'dog'.\",\n",
       " 'By applying these techniques, the text becomes easier for models to analyze and process.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44957436-18f7-4beb-970b-aa286ce73aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'email', 'I.D', 'is', 'maratheharshal005', '@', 'gmail.com']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen2= \"my email I.D is maratheharshal005@gmail.com\"\n",
    "word_tokenize(sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98af29a1-f61c-4a7a-9ba1-09605c2fffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "tok=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd4734c-1300-403e-b6f5-0d6921fe65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=tok(sen1)\n",
    "doc2=tok(sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad1bdfaa-2c0b-465c-9f55-55b4b296e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      "preprocessing\n",
      "is\n",
      "a\n",
      "crucial\n",
      "step\n",
      "in\n",
      "natural\n",
      "language\n",
      "processing\n",
      "(\n",
      "NLP\n",
      ")\n",
      ",\n",
      "where\n",
      "raw\n",
      "text\n",
      "—\n",
      "often\n",
      "noisy\n",
      "and\n",
      "unstructured\n",
      "—\n",
      "is\n",
      "transformed\n",
      "into\n",
      "a\n",
      "format\n",
      "that\n",
      "can\n",
      "be\n",
      "effectively\n",
      "utilized\n",
      "by\n",
      "machine\n",
      "learning\n",
      "algorithms\n",
      ".\n",
      "For\n",
      "example\n",
      ",\n",
      "consider\n",
      "a\n",
      "sentence\n",
      "like\n",
      ":\n",
      "'\n",
      "The\n",
      "quick\n",
      "brown\n",
      "foxes\n",
      ",\n",
      "were\n",
      "jumping\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dogs\n",
      "!\n",
      "'\n",
      "During\n",
      "preprocessing\n",
      ",\n",
      "one\n",
      "might\n",
      "tokenize\n",
      "the\n",
      "sentence\n",
      ",\n",
      "remove\n",
      "stopwords\n",
      "such\n",
      "as\n",
      "'\n",
      "the\n",
      "'\n",
      ",\n",
      "'\n",
      "and\n",
      "'\n",
      ",\n",
      "or\n",
      "'\n",
      "were\n",
      "'\n",
      ",\n",
      "and\n",
      "normalize\n",
      "text\n",
      "by\n",
      "converting\n",
      "everything\n",
      "to\n",
      "lowercase\n",
      ".\n",
      "Additionally\n",
      ",\n",
      "stemming\n",
      "or\n",
      "lemmatization\n",
      "could\n",
      "be\n",
      "applied\n",
      "to\n",
      "words\n",
      "like\n",
      "'\n",
      "jumping\n",
      "'\n",
      ",\n",
      "'\n",
      "foxes\n",
      "'\n",
      ",\n",
      "and\n",
      "'\n",
      "dogs\n",
      "'\n",
      "to\n",
      "reduce\n",
      "them\n",
      "to\n",
      "their\n",
      "base\n",
      "forms\n",
      ":\n",
      "'\n",
      "jump\n",
      "'\n",
      ",\n",
      "'\n",
      "fox\n",
      "'\n",
      ",\n",
      "and\n",
      "'\n",
      "dog\n",
      "'\n",
      ".\n",
      "By\n",
      "applying\n",
      "these\n",
      "techniques\n",
      ",\n",
      "the\n",
      "text\n",
      "becomes\n",
      "easier\n",
      "for\n",
      "models\n",
      "to\n",
      "analyze\n",
      "and\n",
      "process\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7b1fcf8-5171-4cc3-924f-76e96aaf8525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\n",
      "email\n",
      "I.D\n",
      "is\n",
      "maratheharshal005@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59e9b8-c098-4cf0-8a00-059623e8a2be",
   "metadata": {},
   "source": [
    "4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fc09ae4-cd7e-4a52-9682-39c38111026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2960f51e-479d-405b-9863-f9039320eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return\" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c93eb1e-95a9-4957-aad2-50b5ececb4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"text preprocess is a crucial step in natur languag process (nlp), where raw text—often noisi and unstructured—i transform into a format that can be effect util by machin learn algorithms. for example, consid a sentenc like: 'the quick brown foxes, were jump over the lazi dogs!' dure preprocessing, one might token the sentence, remov stopword such as 'the', 'and', or 'were', and normal text by convert everyth to lowercase. additionally, stem or lemmat could be appli to word like 'jumping', 'foxes', and 'dogs' to reduc them to their base forms: 'jump', 'fox', and 'dog'. by appli these techniques, the text becom easier for model to analyz and process.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=\" talk talking talked walking\"\n",
    "stem_words(sen1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2305afa-c631-4386-a709-4c89754c53b8",
   "metadata": {},
   "source": [
    "6. lemmitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8dee725e-4ec3-4705-a6b2-398b9f6248dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,';\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "977fb3de-007f-450a-8102-7e0578abe865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'was',\n",
       " 'running',\n",
       " 'and',\n",
       " 'eating',\n",
       " 'at',\n",
       " 'same',\n",
       " 'time',\n",
       " \"'He\",\n",
       " 'has',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'of',\n",
       " 'swimming',\n",
       " 'after',\n",
       " 'playing',\n",
       " 'long',\n",
       " 'hours',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Sun']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. 'He' has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.',;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca2d72f0-eae6-4281-b801-28bc573778d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'was',\n",
       " 'running',\n",
       " 'and',\n",
       " 'eating',\n",
       " 'at',\n",
       " 'same',\n",
       " 'time',\n",
       " \"'He\",\n",
       " 'has',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'of',\n",
       " 'swimming',\n",
       " 'after',\n",
       " 'playing',\n",
       " 'long',\n",
       " 'hours',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Sun']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sen=nltk.word_tokenize(sentence)\n",
    "for word in sen:\n",
    "    if word in punctuations:\n",
    "        sen.remove(word)\n",
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23be97a9-35f2-4005-b57c-4a4d722681af",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = (\n",
    "    \"The leaves on the trees are falling. The children are playing in the playground. He was reading books all night. The cars were driving quickly on the highway.She enjoys running and jumping.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a33d19a9-2c57-41ae-8e24-ac5fb496e5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "The                 The                 \n",
      "leaves              leave               \n",
      "on                  on                  \n",
      "the                 the                 \n",
      "trees               tree                \n",
      "are                 be                  \n",
      "falling             fall                \n",
      "The                 The                 \n",
      "children            children            \n",
      "are                 be                  \n",
      "playing             play                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "playground          playground          \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "reading             read                \n",
      "books               book                \n",
      "all                 all                 \n",
      "night               night               \n",
      "The                 The                 \n",
      "cars                cars                \n",
      "were                be                  \n",
      "driving             drive               \n",
      "quickly             quickly             \n",
      "on                  on                  \n",
      "the                 the                 \n",
      "highway.She         highway.She         \n",
      "enjoys              enjoy               \n",
      "running             run                 \n",
      "and                 and                 \n",
      "jumping             jump                \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,';\"\n",
    "sentence_words = nltk.word_tokenize(corpus)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8888bb1-6e24-40cf-bbb7-64c34995f40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16733862-f936-4586-8990-822cabd7d2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d95607-189d-4e4c-a763-73285092d961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
